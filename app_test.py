import streamlit as st
from google import genai
from google.genai import types

# 1. ç”»é¢ã®è¨­å®š
st.set_page_config(page_title="ã“ã©ã‚‚è³ªå•ç®±", page_icon="ğŸ£")
st.title("ğŸ£ ã“ã©ã‚‚è³ªå•ç®±")
st.caption("3ã•ã„ã€œ10ã•ã„ã®ã¿ã‚“ãªã® ãã‚‚ã‚“ã« ã“ãŸãˆã‚‹ã‚ˆï¼")

# 2. APIã‚­ãƒ¼ã®è¨­å®š
api_key = st.sidebar.text_input("Gemini API Key", type="password")

if api_key:
    try:
        # SDK v1.0.0ä»¥é™ã®æ›¸ãæ–¹ã«æº–æ‹ 
        client = genai.Client(api_key=api_key)
        
        # 3. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        SYSTEM_PROMPT = """
        ã‚ãªãŸã¯ã€Œã“ã©ã‚‚è³ªå•ç®±ã€ã®å„ªã—ã„å…ˆç”Ÿã§ã™ã€‚
        - 3æ­³ã‹ã‚‰10æ­³ã®å­ä¾›ãŒç†è§£ã§ãã‚‹è¨€è‘‰ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
        - æ¼¢å­—ã«ã¯ï¼ˆï¼‰ã§èª­ã¿ãŒãªã‚’æŒ¯ã‚‹ã‹ã€å¹³ä»®åã‚’å¤šã‚ã«ã—ã¦ãã ã•ã„ã€‚
        - å›ç­”ã¯çŸ­ãã€3æ–‡ä»¥å†…ã§ç­”ãˆã¦ãã ã•ã„ã€‚
        - ä¾‹ãˆè©±ã‚’ä½¿ã„ã€ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹ã‚ˆã†ãªæ•™ãˆæ–¹ã‚’ã—ã¦ãã ã•ã„ã€‚
        - å±é™ºãªã“ã¨ã‚„æ‚ªã„ã“ã¨ã«ã¤ã„ã¦ã¯ã€å„ªã—ãè«­ã—ã¦ãã ã•ã„ã€‚
        """

        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # å±¥æ­´ã®è¡¨ç¤º
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])

        # 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›
        if prompt := st.chat_input("ãªã«ãŒ ã—ã‚ŠãŸã„ï¼Ÿ"):
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # Geminiã‹ã‚‰ã®å›ç­”ã‚’ç”Ÿæˆ
            with st.chat_message("assistant"):
                try:
                    # æœ€æ–°SDKã§ã¯ãƒ¢ãƒ‡ãƒ«åã« 'models/' ã‚’å«ã‚ãªã„ã®ãŒæ­£è§£ãªå ´åˆãŒã‚ã‚Šã¾ã™
                    # ã¾ãŸã€configã®æŒ‡å®šæ–¹æ³•ã‚’ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ã«ã—ã¦ã„ã¾ã™
                    response = client.models.generate_content(
                        model='gemini-1.5-flash', 
                        contents=prompt,
                        config=types.GenerateContentConfig(
                            system_instruction=SYSTEM_PROMPT,
                            temperature=0.7
                        )
                    )
                    
                    if response.text:
                        st.markdown(response.text)
                        st.session_state.messages.append({"role": "assistant", "content": response.text})
                    else:
                        st.error("AIãŒå›ç­”ã‚’ä½œã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                
                except Exception as e:
                    # 404ãŒå‡ºãŸå ´åˆã€è‡ªå‹•ã§åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
                    if "404" in str(e):
                        try:
                            # ãƒ‘ã‚¿ãƒ¼ãƒ³2: models/ ã‚’ä»˜ä¸ã—ãŸå½¢å¼ã§ãƒªãƒˆãƒ©ã‚¤
                            response = client.models.generate_content(
                                model='models/gemini-1.5-flash',
                                contents=prompt,
                                config=types.GenerateContentConfig(system_instruction=SYSTEM_PROMPT)
                            )
                            st.markdown(response.text)
                            st.session_state.messages.append({"role": "assistant", "content": response.text})
                        except Exception as e2:
                            st.error(f"ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚APIã‚­ãƒ¼ã®ç¨®é¡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n{e2}")
                    else:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
                    
    except Exception as init_error:
        st.error(f"åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {init_error}")
else:
    st.info("â† å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã« APIã‚­ãƒ¼ã‚’ã„ã‚Œã¦ã­ï¼")